{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_tokens(text):\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    input_sequences = []\n",
    "    for line in text:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    model.add(LSTM(400, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(400))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (open(\"emily.txt\").read())\n",
    "text = text.lower()\n",
    "words = list(filter(None,re.split('(\\n)| ', text)))\n",
    "\n",
    "words_list = [None] * math.floor((len(words) / 2))\n",
    "\n",
    "for x in range(math.floor((len(words) / 2))):\n",
    "    ind_start = random.randint(0, len(words)-7)\n",
    "    ran = random.randint(4, 7)\n",
    "    words_list[x] = ' '.join(words[ind_start:ind_start+ran])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nibraas/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/nibraas/anaconda3/envs/tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 10)             122270    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 6, 400)            657600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 400)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12227)             4903027   \n",
      "=================================================================\n",
      "Total params: 6,964,497\n",
      "Trainable params: 6,964,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nibraas/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "202362/202362 [==============================] - 23s 112us/step - loss: 7.3050\n",
      "Epoch 2/100\n",
      "202362/202362 [==============================] - 19s 95us/step - loss: 6.9361\n",
      "Epoch 3/100\n",
      "202362/202362 [==============================] - 19s 96us/step - loss: 6.7370\n",
      "Epoch 4/100\n",
      "202362/202362 [==============================] - 19s 96us/step - loss: 6.5988\n",
      "Epoch 5/100\n",
      "202362/202362 [==============================] - 19s 96us/step - loss: 6.5053\n",
      "Epoch 6/100\n",
      "202362/202362 [==============================] - 19s 96us/step - loss: 6.4243\n",
      "Epoch 7/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 6.3478\n",
      "Epoch 8/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 6.2700\n",
      "Epoch 9/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 6.1863\n",
      "Epoch 10/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 6.0925\n",
      "Epoch 11/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 5.9868\n",
      "Epoch 12/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 5.8666\n",
      "Epoch 13/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 5.7465\n",
      "Epoch 14/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 5.6379\n",
      "Epoch 15/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 5.5300\n",
      "Epoch 16/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 5.4265\n",
      "Epoch 17/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 5.3266\n",
      "Epoch 18/100\n",
      "202362/202362 [==============================] - 20s 97us/step - loss: 5.2313\n",
      "Epoch 19/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 5.1377\n",
      "Epoch 20/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 5.0496\n",
      "Epoch 21/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.9643\n",
      "Epoch 22/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.8842\n",
      "Epoch 23/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.8072\n",
      "Epoch 24/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.7341\n",
      "Epoch 25/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 4.6652\n",
      "Epoch 26/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.5970\n",
      "Epoch 27/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.5359\n",
      "Epoch 28/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.4711\n",
      "Epoch 29/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.4147\n",
      "Epoch 30/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.3589\n",
      "Epoch 31/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.3095\n",
      "Epoch 32/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 4.2597\n",
      "Epoch 33/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.2146 1s - loss: - ETA: 0s - loss\n",
      "Epoch 34/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.1685\n",
      "Epoch 35/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.1257\n",
      "Epoch 36/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.0828\n",
      "Epoch 37/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 4.0427\n",
      "Epoch 38/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 4.0057\n",
      "Epoch 39/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.9682\n",
      "Epoch 40/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.9325\n",
      "Epoch 41/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.8983\n",
      "Epoch 42/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.8657\n",
      "Epoch 43/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 3.8380\n",
      "Epoch 44/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.8015\n",
      "Epoch 45/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 3.7699\n",
      "Epoch 46/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.7480\n",
      "Epoch 47/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 3.7180\n",
      "Epoch 48/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.6896\n",
      "Epoch 49/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 3.6646\n",
      "Epoch 50/100\n",
      "202362/202362 [==============================] - 20s 99us/step - loss: 3.6405\n",
      "Epoch 51/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.6153\n",
      "Epoch 52/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.5894 0s - loss\n",
      "Epoch 53/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.5687\n",
      "Epoch 54/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.5412 0s - los\n",
      "Epoch 55/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.5231\n",
      "Epoch 56/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.5004\n",
      "Epoch 57/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.4768\n",
      "Epoch 58/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.4561\n",
      "Epoch 59/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.4356\n",
      "Epoch 60/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.4175\n",
      "Epoch 61/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.3946\n",
      "Epoch 62/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.3753\n",
      "Epoch 63/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.3571\n",
      "Epoch 64/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.3369\n",
      "Epoch 65/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.3230\n",
      "Epoch 66/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 3.3003\n",
      "Epoch 67/100\n",
      "202362/202362 [==============================] - 20s 100us/step - loss: 3.2833\n",
      "Epoch 68/100\n",
      "202362/202362 [==============================] - 21s 102us/step - loss: 3.2697\n",
      "Epoch 69/100\n",
      "202362/202362 [==============================] - 20s 101us/step - loss: 3.2493\n",
      "Epoch 70/100\n",
      "202362/202362 [==============================] - 21s 102us/step - loss: 3.2317\n",
      "Epoch 71/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.2192\n",
      "Epoch 72/100\n",
      "202362/202362 [==============================] - 21s 103us/step - loss: 3.2028\n",
      "Epoch 73/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.1865\n",
      "Epoch 74/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.1671\n",
      "Epoch 75/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.1518\n",
      "Epoch 76/100\n",
      "202362/202362 [==============================] - 21s 103us/step - loss: 3.1397\n",
      "Epoch 77/100\n",
      "202362/202362 [==============================] - 21s 102us/step - loss: 3.1256\n",
      "Epoch 78/100\n",
      "202362/202362 [==============================] - 21s 102us/step - loss: 3.1094\n",
      "Epoch 79/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.0935\n",
      "Epoch 80/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.0800\n",
      "Epoch 81/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.06451\n",
      "Epoch 82/100\n",
      "202362/202362 [==============================] - 21s 102us/step - loss: 3.0481\n",
      "Epoch 83/100\n",
      "202362/202362 [==============================] - 21s 104us/step - loss: 3.0356\n",
      "Epoch 84/100\n",
      "202362/202362 [==============================] - 21s 103us/step - loss: 3.0258\n",
      "Epoch 85/100\n",
      "202362/202362 [==============================] - 21s 103us/step - loss: 3.0130\n",
      "Epoch 86/100\n",
      "202362/202362 [==============================] - 20s 100us/step - loss: 2.9950\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202362/202362 [==============================] - 20s 100us/step - loss: 2.98723s - loss - ETA: 3s - loss: \n",
      "Epoch 88/100\n",
      "202362/202362 [==============================] - 21s 103us/step - loss: 2.9727\n",
      "Epoch 89/100\n",
      "202362/202362 [==============================] - 21s 103us/step - loss: 2.9603\n",
      "Epoch 90/100\n",
      "202362/202362 [==============================] - 21s 102us/step - loss: 2.9445\n",
      "Epoch 91/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.9342\n",
      "Epoch 92/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.9235\n",
      "Epoch 93/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.9090\n",
      "Epoch 94/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.8975\n",
      "Epoch 95/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.8869\n",
      "Epoch 96/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.8736\n",
      "Epoch 97/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.8620\n",
      "Epoch 98/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.8477\n",
      "Epoch 99/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.8402\n",
      "Epoch 100/100\n",
      "202362/202362 [==============================] - 20s 98us/step - loss: 2.8289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6703379e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=100, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = generate_text(\"Nibraas\", 50, model, max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nibraas The Sun And Fog Before All The Deepest Cellar That Were The Thrill Of The Soul That The Dirt Are The Same Of The Nut Combined Too Cold The East Moment Is No Man Is The Jay That Juicy Satisfies An Single Grace A Robins Larder Had Been The Kmd'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
